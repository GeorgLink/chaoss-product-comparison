
Last modified: 2017-11-27

| Author | Email(s) |
| ------ | ----- |
| Jason T Clark | jtclark@linux.vnet.ibm.com |
| Georg J. P. Link | glink@unomaha.edu |
| Matt Germonprez | germonprez@gmail.com |


# Introduction

As the [mission statement](https://chaoss.community/about/governance/) describes, the Linux Foundation Community Health Analytics for Open Source Software (CHAOSS) project aims to _"produce integrated, open source software for analyzing software development"_, and _"establish implementation-agnostic metrics for measuring community activity, contributions, and health"_.

To achieve this goal, our community members are defining which key metrics accurately evaluate the overall health of open-source projects. These metrics will serve as the measurements monitored by the product(s) developed by the CHAOSS community. In addition to this work, the following research provides a comparative analysis of the existing tools and/or production sites that currently provide health-related metrics for open-source projects using dashboard views.

## Purpose

The purpose of this research is to provide a side-by-side comparison of the tools and community sites that reflect the health of the open-source software projects they monitor. By conducting this research, the CHAOSS community is afforded the opportunity to assess the following from each of the community health monitoring tools assesed:

- Key contribution metrics
- Components/Features available
- Ease of use/implementation

The open-source communities represented by the members of the CHAOSS project also have the opportunity to use this research as a means to determine which of these metrics, components, features community health

# Methodology

The data collected in this report has been gathered using one or more of the following methods:

- General observation of the public facing production site
- Local installation of the software project
- Referencing the information provided in the product documentation

The method of data collection and analysis is mentioned to ensure that the reader understands the limitations of the information collected in this report. In some cases, the analysis could prove to be more accurate under more appropriate use-cases and scenarios.

# Brief Overview

Our analysis begins with a brief overview of the production sites and/or software projects researched. The information provided below introduces each tool individually and summarizes the metrics, components, usage, and implementation details.

## Production Site: [Stackalytics](http://stackalytics.com/)

![stackalytics-logo](http://stackalytics.com/static/images/stackalytics_logo.png)

The Stackalytics (OpenStack Analytics) site was developed my the Mirantis corporation in 2013 to provide OpenStack with an analytics tool that collects code contributions data, vendor drivers information, and a community members directory. As [defined by Mirantis](https://www.mirantis.com/blog/stackalytics-com-whos-growing-the-openstack-pie/), _"Stackalytics is a data visualization tool that collects data from GitHub and presents it in an array of useful forms."_

### Code Contribution Metrics

From the Stackalytics site, a number of key metrics are collected from the code contributions made by the OpenStack commmunity. They include:

- Commits
- Completed Blueprints
- Drafted Blueprints
- Emails
- Filed Bugs
- Lines of Code
- Patch Sets
- Person-day effort
- Resolved Bugs
- Reviews
- Translations

TODO: DEFINE EACH OF THE METRICS HERE

### Code Contribution Filters

The data provided by each of these metrics can be filtered in a number of ways. They are as follows:

- Release
- Project Type
- Module
- Company
- Contributor

TODO: DEFINE EACH OF THE FILTERS HERE



## Software: [Apache Kibble](https://kibble.apache.org/)


# Comparative Analysis
